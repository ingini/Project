import boto3
import xml.etree.ElementTree as ET
import time
import os
import json
import traceback
import pandas as pd
import signal
from pytz import timezone
from datetime import datetime, timedelta
from botocore.exceptions import ClientError
from datetime import datetime
from pyarrow import Table
from pyarrow.parquet import write_table
from timeout import LambdaTimeoutException


parquet_upload_bucket = 'hm-datalake-staging.ver01'

s3_client = boto3.client('s3', region_name='ap-northeast-2')
sqs = boto3.client('sqs', region_name='ap-northeast-2')

collections_with_partition = {'carplat': ['eventDriving', 'gwCallResultHistory', 'objectHistory', 'objectHistoryV1.1', 'notification_history', 'tripAlarmHistory','tripEventHistory'],
'peopleCar': ['eventDriving', 'gwCallResultHistory', 'objectHistoryV1.1', 'notification_history', 'tripAlarmHistory', 'tripEventHistory']}

def timeout_handler(_signal, _frame):
    raise LambdaTimeoutException('Time limit exceeded')
    
def flatten_json_to_dict(so_id, collection_name, y):
    out = {}
    def flatten(x, name=''):
        ## skip too long arrays and separate them to another table
        if ('bookingExtendHistories' in name or 'bookingStatusHistories' in name) and collection_name == 'bookings':
            pass
        elif 'targetAttributes' in name  and collection_name == 'bookings' and so_id == 'carplat':
            pass
        elif 'coordinates' in name and so_id == 'peopleCar' and 'zoneGroupV1.1' in collection_name:
            out[name[:-1]] = x
        elif type(x) is dict:
            for a in x:
                flatten(x[a], name + a + '_')
        elif type(x) is list:
            i = 0
            for a in x:
                flatten(a, name + str(i) + '_')
                i += 1
        else:
            name = name.replace('_id_$oid','_id')
            if name.startswith('mcuID'):
                name = '_mcuID_'
            elif name.startswith('vehicleInfoID'):
                name = '_vehicleInfoID_'
            elif name.lower().startswith('objecttripinfoatstart__class'):
                name = '_objectTripInfoAtStart__class_'
            out[name[:-1]] = x
    flatten(y)
    return out


def lambda_handler(event, context):
    try:
        signal.alarm(int(context.get_remaining_time_in_millis() / 1000) - 1)
    
        bucket = event['Records'][0]['s3']['bucket']['name'] # hm-datalake-prestaging.ver01
        key = event['Records'][0]['s3']['object']['key'].replace('%3D', '=') # raw/raidea/carplat/json_new/billsV1.1/yy=2022/mm=08/dd=09/billsV1.1.json
        
        # bucket = 'hm-datalake-prestaging.ver01'
        # key = 'raw/raidea/carplat/json_new/billsV1.1/yy=2022/mm=08/dd=09/billsV1.1.json'
        
        key_split = key.split('/')
        
        # json is located in root directory of each collection that means it is the latest json and it does not have to be parsed
        if len(key_split) < 7:
            print('triggered file is the latest json. skip')
            
            signal.alarm(0)
        
            return {
                'statusCode': 200,
                'body': json.dumps('success')
            }
        
        json_file_name = key_split[-1]
        
        collection_name = key_split[4]
        parquet_file_name = json_file_name.rsplit('.', 1)[0] + '.parquet'
        
        json_file_path = '/tmp/{}'.format(json_file_name)
        parquet_file_path = '/tmp/{}'.format(parquet_file_name)
        so_id = key_split[2]
        
        s3_client.download_file(bucket, key, json_file_path)
            
        # process parquet
        if collection_name in collections_with_partition[so_id]:
            partition_yy = key_split[5]
            partition_mm = key_split[6]
            partition_dd = key_split[7]
            parquet_upload_key = 'raidea/{}/parquet_new/{}/{}/{}/{}/{}'.format(so_id, collection_name, partition_yy, partition_mm, partition_dd, parquet_file_name)
        else:
            parquet_upload_key = 'raidea/{}/parquet_new/{}/{}'.format(so_id, collection_name, parquet_file_name)
            
            
        print('processing info: ', collection_name, json_file_name, parquet_file_name, json_file_path, parquet_file_path, parquet_upload_key)
    
        # json -> array of dict -> dataframe -> parquet
        # from json to array of dict
        array = []
        with open(json_file_path,'r') as f:
    
            # convert json into array of dict
            for line in f:
                data = json.loads(line)
    
                flatten = flatten_json_to_dict(so_id, collection_name, data)
    
                for k, v in flatten.items():
                    flatten[k] = str(v)
                    if not str(v):
                        flatten[k] = None
                array.append(flatten)
            
            print('converted json into array of dict')
    
    
            # convert array of dict into dataframe
            df = pd.DataFrame.from_dict(array)
            
            print('converted array of dict into dataframe')
    
            # convert dataframe into parquet
            table = Table.from_pandas(df)
            print('converting {} rows from {} into {}'.format(len(table), json_file_path, parquet_file_path))                
            write_table(table, parquet_file_path)
            array = []
    
            # copy parquet into staging that is referenced by AWS Athena
            s3_client.upload_file(parquet_file_path, parquet_upload_bucket, parquet_upload_key)
    
            print('parquet done collection name = ', collection_name)
            
        signal.alarm(0)
        
        return {
            'statusCode': 200,
            'body': json.dumps('success')
        }
            
    except LambdaTimeoutException as e:
        msg = sqs.send_message(QueueUrl="https://sqs.ap-northeast-2.amazonaws.com/284145144652/Lambda-Daily-Error-Report", MessageBody="Timeout,{},{}".format(context.function_name, context.aws_request_id))
        return {'statusCode': 400, 'body': str(e)}
    except Exception as e:
        traceback.print_exc()
        msg = sqs.send_message(QueueUrl="https://sqs.ap-northeast-2.amazonaws.com/284145144652/Lambda-Daily-Error-Report", MessageBody="Error,{},{},{}".format(context.function_name, context.aws_request_id, traceback.format_exc()))
        return {'statusCode': 400, 'body': str(e)}  