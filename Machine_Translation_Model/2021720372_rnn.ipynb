{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2021720372_rnn.ipynb","provenance":[{"file_id":"1uq11lqSl02-75H8aqej8Y30nZAOy5CwW","timestamp":1650952550057},{"file_id":"1LU3bk03EYWf00y_Ag-WcoBbqD0P4JHHi","timestamp":1612862379709}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"1w8ew706HC5C"},"source":["# 기계 번역 모델 학습 \n","\n","이번 실습에서는 기계 번역을 수행하는 RNN 기반 모델의 학습에 대하여 살펴보겠습니다.\n","\n","기계 번역이란 하나의 언어로 적혀져있는 문장을 다른 언어의 문장으로 번역하는 분야를 뜻합니다.\n","\n","룰에 따라 번역을 하는 단순한 기계 번역부터 딥러닝 모델을 이용한 최신 모델까지 다양하게 있습니다.\n","\n","이번 실습에서는 딥러닝 모델 중 그 기초가 되는 RNN을 이용한 Sequence to Sequence 모델을 만들겠습니다.\n","\n","그리고 이를 영어-한국어 병렬 데이터를 이용하여 학습하는 것까지 진행하겠습니다."]},{"cell_type":"markdown","metadata":{"id":"Qjj7XEd3BuDy"},"source":["## 데이터 파일 다운로드\n","\n","데이터 파일을 다운로드 하기 위해 특수 명령어인 gdown을 사용하였습니다.\n","\n","기계 번역 학습을 위해서는 한 언어로 적혀진 문장과 같은 뜻을 가진 다른 언어로 적혀진 문장이 필요합니다.\n","\n","그렇기에 이러한 데이터를 병렬 데이터라고 부릅니다.\n","\n","예를 들어 아래와 같은 문장들이 데이터입니다.\n","\n","- 영어: Sign here please.\n","\n","- 한국어: 여기에 서명하세요.\n","\n","이러한 데이터를 통해 기계 번역 모델은 학습을 합니다.\n","\n","그리고 테스트 때 새로운 문장을 받으면 그에 해당하는 다른 언어의 문장으로 바꿔야 합니다.\n","\n","- 입력: Do you understand?\n","\n","- 출력: 이해하니?\n","\n","이러한 병렬 데이터는 여러 있지만 이번 실습에서는 영화와 TV 자막으로 구축한 데이터인 `OpenSubtitles`를 사용하겠습니다.\n","\n","데이터 출처: https://opus.nlpl.eu/OpenSubtitles2018.php"]},{"cell_type":"code","metadata":{"id":"B3hwEF6GAiEm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651109771349,"user_tz":-540,"elapsed":3591,"user":{"displayName":"빅데이터학과/한대건","userId":"11786315672058136517"}},"outputId":"8d584dfe-0b82-409c-9776-79f331157186"},"source":["!gdown --id 1DGeE1UY8nAhlYwLIIyNRHJ-SlVaxcURU\n","\n","!gdown --id 1GPUgAlDKn0Wk1TqILJwzqeS84eBiOngU"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n","  category=FutureWarning,\n","Downloading...\n","From: https://drive.google.com/uc?id=1DGeE1UY8nAhlYwLIIyNRHJ-SlVaxcURU\n","To: /content/train_en_ko.csv\n","100% 145k/145k [00:00<00:00, 76.2MB/s]\n","/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n","  category=FutureWarning,\n","Downloading...\n","From: https://drive.google.com/uc?id=1GPUgAlDKn0Wk1TqILJwzqeS84eBiOngU\n","To: /content/valid_en_ko.csv\n","100% 72.5k/72.5k [00:00<00:00, 72.2MB/s]\n"]}]},{"cell_type":"markdown","metadata":{"id":"S03tCoPkG6vr"},"source":["`train_en_ko.csv` 파일을 열어보면 한 라인에 두 개의 열이 있습니다. \n","\n","첫 번째 열에는 영어 문장이 적혀있고 두 번째 열에는 그 문장과 같은 뜻을 가지는 한국어 문장이 있습니다.\n","\n","이렇게 병렬로 문장이 데이터로 있는 것을 알 수 있습니다."]},{"cell_type":"code","metadata":{"id":"xumuS251GFJO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651109771349,"user_tz":-540,"elapsed":4,"user":{"displayName":"빅데이터학과/한대건","userId":"11786315672058136517"}},"outputId":"ba1a180a-9973-4ba9-a7fc-6435142fc21c"},"source":["with open(\"train_en_ko.csv\") as csv_f:\n","    head = \"\\n\".join([next(csv_f) for x in range(5)])\n","print(head)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\"Through the snow and sleet and hail, through the blizzard, through the gales, through the wind and through the rain, over mountain, over plain, through the blinding lightning flash, and the mighty thunder crash,\",\"폭설이 내리고 우박, 진눈깨비가 퍼부어도 눈보라가 몰아쳐도 강풍이 불고 비바람이 휘몰아쳐도\"\n","\n","\"ever faithful, ever true, nothing stops him, he'll get through.\",우리의 한결같은 심부름꾼 황새 아저씨 가는 길을 그 누가 막으랴!\n","\n","Look out for Mr Stork That persevering chap,황새 아저씨를 기다리세요\n","\n","He'll come along and drop a bundle in your lap,찾아와 선물을 주실 거예요\n","\n","You may be poor or rich It doesn't matter which,가난하든 부자이든 상관이 없답니다\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"ZvdTJtt1CcRU"},"source":["## 라이브러리 로드\n","\n","코드 실행에 필요한 라이브러리를 설치하고 로드합니다."]},{"cell_type":"code","metadata":{"id":"2tmTo5O9aHtH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651109779984,"user_tz":-540,"elapsed":8637,"user":{"displayName":"빅데이터학과/한대건","userId":"11786315672058136517"}},"outputId":"db66109d-fcb8-441f-d4f2-26b754acc30f"},"source":["!pip install konlpy"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting konlpy\n","  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n","\u001b[K     |████████████████████████████████| 19.4 MB 1.4 MB/s \n","\u001b[?25hCollecting JPype1>=0.7.0\n","  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n","\u001b[K     |████████████████████████████████| 448 kB 46.7 MB/s \n","\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n","Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.21.6)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (4.2.0)\n","Installing collected packages: JPype1, konlpy\n","Successfully installed JPype1-1.3.0 konlpy-0.6.0\n"]}]},{"cell_type":"code","metadata":{"id":"RgnaF87tCfhj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651109785490,"user_tz":-540,"elapsed":5509,"user":{"displayName":"빅데이터학과/한대건","userId":"11786315672058136517"}},"outputId":"f7a489f4-388c-430a-fd04-370642bed330"},"source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn as nn\n","import torch.optim\n","from nltk.tokenize import word_tokenize\n","import os\n","import nltk\n","import codecs\n","import csv\n","from konlpy.tag import Okt\n","import torch.nn.functional as F\n","nltk.download('punkt')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"IlUeGHEJPZeV"},"source":["\n","## 모델 클래스 정의\n","\n","pytorch는 딥러닝 모델의 forward path를 정의할 때는 반드시 `nn.Module` 클래스로부터 상속을 받아 새로운 클래스로 만들어야 합니다.\n","\n","그리고 그 forward path를 정의하기 위해 반드시 `forward` 함수를 정의하여야 합니다.\n","\n","이번 실습에서는 RNN을 이용한 기계 번역 모델이기에 그에 적합한 클래스를 작성하였습니다.\n","\n","- 문제 1. `GRUMT` 클래스 내 model 구성에 있어 RNN으로 GRU를 사용하겠습니다. `encoder_rnn`과 `decoder_rnn`에 GRU를 정의하세요.\n","  - 힌트 1) GRU의 크기는 `hidden_size`에 맞춰주세요.\n","  - 힌트 2) 간단히 한 방향으로만 움직이는 GRU를 만들어주세요."]},{"cell_type":"code","metadata":{"id":"pia7XebEPbS1"},"source":["class GRUMT(nn.Module):\n","    # GRU 기반 MT 클래스를 정의합니다. Pytorch는 모델을 구성할 때 반드시 nn.Module 클래스를 상속받은 후 이를 토대로 만듭니다.\n","    def __init__(self, input_size, hidden_size, output_size, max_length, device, dropout_p=0.1):\n","        # 클래스의 첫 시작인 함수입니다. 여기서 모델에 필요한 여러 변수들을 정의합니다.\n","        super(GRUMT, self).__init__()\n","\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.output_size = output_size\n","        self.dropout_p = dropout_p\n","        self.max_length = max_length\n","        self.device = device\n","\n","        # Encoder 부분\n","        self.encoder_embedding = nn.Embedding(input_size, hidden_size)\n","        # <ToDo>: encoder를 GRU로 정의하세요.\n","        # self.encoder_rnn = None\n","        #self.encoder_rnn = nn.GRU(self.input_size,self.hidden_size,num_layers=1,bias=True,batch_first=True,bidirectional=True)\n","        self.encoder_rnn = nn.GRU(self.hidden_size,self.hidden_size)\n","\n","        # Decoder 부분\n","        # <ToDo>: decoder를 GRU로 정의하세요.\n","        # self.decoder_rnn = None\n","        #self.decoder_rnn = nn.GRU(self.output_size,self.hidden_size,num_layers=1,bias=True,batch_first=True,bidirectional=True) drop_last=True \n","        self.decoder_rnn = nn.GRU(self.hidden_size,self.hidden_size)\n","        self.decoder_embedding = nn.Embedding(self.output_size, self.hidden_size)\n","        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n","        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n","        self.dropout = nn.Dropout(self.dropout_p)\n","        self.out = nn.Linear(self.hidden_size, self.output_size)\n","\n","        self.loss = nn.NLLLoss()\n","\n","    def _encoder(self, input_tensor, input_length):\n","        # forward 함수 중 첫 번째 부분인 encoder에 대한 함수입니다.\n","        encoder_hidden = self._init_hidden()\n","\n","        encoder_outputs = torch.zeros(self.max_length, self.hidden_size, device=self.device)\n","\n","        # input_tensor의 길이만큼 하나씩 GRU를 통과시키고 그 결과를 저장합니다.\n","        for idx in range(input_length):\n","            input_tensor_step = input_tensor[idx]\n","            embedded = self.encoder_embedding(input_tensor_step).view(1, 1, -1)\n","            encoder_output, encoder_hidden = self.encoder_rnn(embedded, encoder_hidden)\n","            encoder_outputs[idx] = encoder_output[0, 0]\n","\n","        return encoder_outputs, encoder_hidden\n","\n","    def _decoder(self, target_tensor, target_length, encoder_hidden, encoder_outputs):\n","        # forward 함수 중 두 번째 부분인 decoder에 대한 함수입니다.\n","        # decoder의 입력은 특수 문자인 SOS입니다.\n","        decoder_input = torch.tensor([[SOS_token]], device=self.device)\n","        decoder_hidden = encoder_hidden\n","\n","        loss_sum = 0\n","        # 번역할 문장의 길이만큼 단어를 생성합니다. \n","        # 단어 생성은 주어진 언어 사전에 있는 단어 중 하나를 선택하는 classification 문제와 동일합니다.\n","        for di in range(target_length):\n","            embedded = self.decoder_embedding(decoder_input).view(1, 1, -1)\n","            embedded = self.dropout(embedded)\n","\n","            # encoder의 결과와 decoder의 hidden을 결합하여 현재 생성할 단어에 영향을 많이 주는 attention을 구합니다.\n","            decoder_attention = F.softmax(self.attn(torch.cat((embedded[0], decoder_hidden[0]), 1)), dim=1)\n","            attn_applied = torch.bmm(decoder_attention.unsqueeze(0), encoder_outputs.unsqueeze(0))\n","\n","            output = torch.cat((embedded[0], attn_applied[0]), 1)\n","            output = self.attn_combine(output).unsqueeze(0)\n","\n","            output = F.relu(output)\n","            output, decoder_hidden = self.decoder_rnn(output, decoder_hidden)\n","\n","            decoder_output = F.log_softmax(self.out(output[0]), dim=1)\n","\n","            # decoder를 거쳐 나온 출력 중 가장 높은 값을 가지는 단어를 찾습니다.\n","            # 찾은 단어는 다음 반복문의 입력 단어가 됩니다.\n","            _, topi = decoder_output.topk(1)\n","            decoder_input = topi.squeeze().detach()\n","            target_output = torch.tensor([target_tensor[di]], device=self.device)\n","            \n","            # 그리고 그 단어와 실제 단어의 차이를 loss로 정의합니다.\n","            loss_sum += self.loss(decoder_output, target_output)\n","            if decoder_input.item() == EOS_token:\n","                break\n","\n","        return loss_sum\n","\n","    def forward(self, input_tensor, input_length, target_tensor, target_length):\n","        # 모델의 forward feed를 수행하는 함수입니다.\n","        # 영어 문장과 한국어 문장 두 개를 받아 영어 문장에서 한국어 문장을 만드는 seq2seq 모델입니다.\n","        # Encoder 파트\n","        encoder_outputs, encoder_hidden = self._encoder(input_tensor, input_length)\n","\n","        # Decoder 파트\n","        loss_sum = self._decoder(target_tensor, target_length, encoder_hidden, encoder_outputs)\n","\n","        return loss_sum\n","\n","    def _init_hidden(self):\n","        # encoder와 decoder 둘 다 처음에 가지는 hidden입니다. 간단히 0으로 시작합니다.\n","        return torch.zeros(1, 1, self.hidden_size, device=self.device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f0C0fY54ZDmU"},"source":["## 데이터 클래스 정의\n","\n","이번 실습에서는 데이터를 불러올 때 토큰을 만들 수 있도록 데이터 클래스와 이를 만드는 함수 `make_data_loader`를 정의하겠습니다.\n","\n","이에 앞서 언어별 단어 사전을 만드는 클래스를 정의하여 사용하겠습니다.\n","\n","이전 실습에서는 `Field` 내 `build_vocab` 함수를 통해 단어 사전을 만들었습니다. 이를 이번에는 직접 구현해보도록 하겠습니다."]},{"cell_type":"markdown","metadata":{"id":"JgRv_KuZpEzA"},"source":["단어 사전을 만들 때 있어 기본적으로 가지는 특수한 단어는 3개가 있습니다.\n","\n","- SOS: Start of the sentence, 문장의 시작을 알리는 단어\n","- EOS: End of the sentence, 문장의 끝을 알리는 단어\n","- UNK: Unknown word, 모르는 단어\n","\n","SOS와 EOS는 각각 문장의 처음과 끝을 알리는 단어입니다.\n","\n","decoder는 SOS를 처음 입력으로 받아 그로부터 문장 생성을 시작합니다.\n","\n","그리고 EOS를 생성하게 되면 단어 생성을 멈추는 것입니다.\n","\n","UNK는 모르는 단어로 out of vocabulary를 표현할 때 사용합니다.\n","\n","즉, training 데이터에는 나타나지 않지만 validation 혹은 test 데이터에 나타난 단어가 있을 때 그 단어에 대한 표시를 하는 것입니다.\n","\n","이러한 단어를 무시하는 것도 하나의 방법입니다. 하지만 그런 경우 모르는 단어가 존재한다는 정보가 사라지기에 이를 보완하고자 특수 단어로 표시하는 것입니다."]},{"cell_type":"code","metadata":{"id":"xH-Goaky8J5w"},"source":["class LangDic:\n","    # 언어마다 단어 사전을 정의합니다.\n","    def __init__(self, name, tokenizer):\n","        # 클래스의 첫 시작인 함수입니다. 여기서 모델에 필요한 여러 변수들을 정의합니다.\n","        self.name = name\n","        self.word2index = {}\n","        self.word2count = {}\n","        self.index2word = {0: \"SOS\", 1: \"EOS\", 2: \"UNK\"}\n","        self.n_words = 2\n","        self.tokenizer = tokenizer\n","\n","    def add_sentence(self, sentence):\n","        # 문장을 받아 문장에서 단어를 확인합니다.\n","        for word in self.tokenizer(sentence):\n","            self.add_word(word)\n","\n","    def add_word(self, word):\n","        # 단어를 보고 그 단어가 사전에 존재하는지 아닌지를 살펴봅니다.\n","        # 존재하지 않는 경우 단어를 사전에 등록합니다.\n","        if word not in self.word2index:\n","            self.word2index[word] = self.n_words\n","            self.word2count[word] = 1\n","            self.index2word[self.n_words] = word\n","            self.n_words += 1\n","        else:\n","            self.word2count[word] += 1\n","\n","    def sentence2tensor(self, sentence, max_len):\n","        # 가지고 있는 사전을 바탕으로 문장을 tensor의 형태로 바꿉니다.\n","        # tensor 내 들어있는 값은 단어 index이며 이를 통해 모델의 임베딩에 입력으로 줄 수 있습니다.\n","        indexes = list()\n","        for word in self.tokenizer(sentence):\n","            try:\n","                indexes.append(self.word2index[word])\n","            except KeyError:\n","                indexes.append(UNK_token)\n","\n","        indexes.append(EOS_token)\n","        len_sen = len(indexes)\n","        if len_sen > max_len:\n","            indexes = indexes[:max_len-1]\n","            indexes.append(EOS_token)\n","            len_sen = max_len\n","\n","        index_tensor = torch.tensor(indexes)\n","        return index_tensor, len_sen"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zoyLsQU08N2D"},"source":["def make_dic(dataset_path):\n","    # 사전을 만드는 함수입니다.\n","    data_pairs = load_file(dataset_path)\n","\n","    # 영어의 경우 NLTK tokenizer를\n","    # 한국어의 경우 Konlpy 내 Open Korean Text tokenizer를 이용합니다.\n","    eng_tokenizer = word_tokenize\n","    kor_tokenizer = Okt().morphs\n","\n","    eng_dic = LangDic('en', eng_tokenizer)\n","    kor_dic = LangDic('ko', kor_tokenizer)\n","\n","    for eng_sen, kor_sen in data_pairs:\n","        eng_dic.add_sentence(eng_sen)\n","        kor_dic.add_sentence(kor_sen)\n","\n","    return eng_dic, kor_dic\n","\n","def load_file(dataset_path):\n","    # 데이터를 읽는 함수입니다.\n","    data_pairs = list()\n","    # 데이터 파일의 내용을 불러와 영어 문장과 한국어 문장을 모아 리스트에 넣습니다.\n","    with codecs.open(dataset_path, \"r\", \"utf-8\") as csv_f:\n","        csv_reader = csv.reader(csv_f)\n","        for one_row in csv_reader:\n","            data_pairs.append(one_row)\n","\n","    return data_pairs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jEF1pQoYZRqA"},"source":["class MTDataset(Dataset):\n","    # pytorch로 데이터를 불러오기 위해서 Dataset 클래스를 상속받아 새로운 클래스를 만듭니다.\n","    def __init__(self, data_pairs, eng_dic, kor_dic, max_len):\n","        super(MTDataset, self).__init__()\n","\n","        # 데이터를 파일로부터 읽어 이를 전달 받습니다.\n","        self.max_len = max_len\n","        self.pair_data = list()\n","\n","        # 데이터 내 문장을 미리 정의한 사전에 기반하여 tensor로 바꿉니다.\n","        for eng_sen, kor_sen in data_pairs:\n","            eng_sen_words, eng_sen_len = eng_dic.sentence2tensor(eng_sen, max_len)\n","            kor_sen_words, kor_sen_len = kor_dic.sentence2tensor(kor_sen, max_len)\n","            self.pair_data.append((eng_sen_words, eng_sen_len, kor_sen_words, kor_sen_len))\n","\n","        self.data_len = len(self.pair_data)\n","\n","    def __getitem__(self, idx):\n","        # idx번째 데이터를 반환합니다.\n","        eng_sen_words, eng_sen_len, kor_sen_words, kor_sen_len = self.pair_data[idx]\n","\n","        return eng_sen_words, eng_sen_len, kor_sen_words, kor_sen_len\n","\n","    def __len__(self):\n","        return self.data_len"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nATcgR-UZV-I"},"source":["def make_data_loader(dataset_path, eng_dic, kor_dic, max_len, batch_size):\n","    # DataLoader를 만들어서 데이터를 불러오도록 합니다.\n","    data_pairs = load_file(dataset_path)\n","\n","    # 앞서 정의한 MTDataset 클래스에 해당 데이터를 넣습니다.\n","    ds = MTDataset(data_pairs, eng_dic, kor_dic, max_len)\n","\n","    # 만들어진 MTDataset 클래스를 DataLoader에 넣고 batch 크기를 전달해줍니다.\n","    return DataLoader(ds, batch_size=batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4ZhhZRkJChKT"},"source":["## train 함수\n","\n","해당 함수에서는 정의된 `model` 클래스의 인스턴스를 가져와서 이를 train data로 학습시킵니다. 그리고 validation data로 학습 중간에 성능을 평가합니다."]},{"cell_type":"code","metadata":{"id":"hYucrg5wCjyA"},"source":["def train(model, device, optimizer, train_loader, valid_loader, num_epochs):\n","    # 학습에 필요한 변수들을 기본적으로 정의합니다.\n","    running_loss = 0.0\n","    global_step = 0\n","    eval_every = 100\n","\n","    # model에게 학습이 진행됨을 알려줍니다.\n","    model.train()\n","    # num_epochs만큼 epoch을 반복합니다.\n","    for epoch in range(num_epochs):\n","        # train_loader를 읽으면 정해진 데이터를 읽어옵니다.\n","        for input_tensor, input_length, target_tensor, target_length in train_loader:\n","            # 데이터를 GPU로 옮깁니다.\n","            input_tensor = input_tensor[0].to(device)\n","            target_tensor = target_tensor[0].to(device)\n","\n","            # model을 함수처럼 호출하면 model에서 정의한 forward 함수가 실행됩니다.\n","            # 즉, 데이터를 모델에 집어넣어 forward방향으로 흐른 후 그 결과를 받습니다.\n","            loss_sum = model(input_tensor, input_length, target_tensor, target_length)\n","\n","            # 최적화 수행\n","            optimizer.zero_grad()\n","            loss_sum.backward()\n","            optimizer.step()\n","\n","            running_loss += loss_sum.item() / target_length.item()\n","            global_step += 1\n","\n","            if global_step % eval_every == 0:\n","                # 100번에 한 번으로 validation 데이터를 이용하여 성능을 검증합니다.\n","                print_loss_avg = running_loss / eval_every\n","\n","                average_valid_loss = evaluate(model, device, valid_loader)\n","\n","                # 검증이 끝난 후 다시 모델에게 학습을 준비시킵니다.\n","                model.train()\n","                running_loss = 0.0\n","\n","                # 결과 출력\n","                print('Epoch {}, Step {}, Train Loss: {:.4f}, Valid Loss: {:.4f}'\n","                      .format(epoch + 1, global_step, print_loss_avg, average_valid_loss))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zz8Q3e9iQroL"},"source":["\n","\n","\n","\n","\n","\n","\n","## evaluate 함수\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","해당 함수에서는 validation data를 이용하여 학습된 `model`을 평가합니다.\n"]},{"cell_type":"code","metadata":{"id":"5PcidZwmQuFu"},"source":["def evaluate(model, device, valid_loader):\n","    # 학습 중 모델을 평가합니다.\n","    # 모델에게 학습이 아닌 평가를 할 것이라고 알립니다.\n","    model.eval()\n","    valid_running_loss = 0.0\n","\n","    # 학습이 아니기에 최적화를 하지 않는다는 환경을 설정합니다.\n","    with torch.no_grad():\n","        # validation 데이터를 읽습니다.\n","        for input_tensor, input_length, target_tensor, target_length in valid_loader:\n","            input_tensor = input_tensor[0].to(device)\n","            target_tensor = target_tensor[0].to(device)\n","\n","            # model을 함수처럼 호출하면 model에서 정의한 forward 함수가 실행됩니다.\n","            # 즉, 데이터를 모델에 집어넣어 forward방향으로 흐른 후 그 결과를 받습니다.\n","            loss_sum = model(input_tensor, input_length, target_tensor, target_length)\n","\n","            # validation 데이터의 loss, 즉 모델의 출력과 실제 데이터의 차이를 구합니다.\n","            valid_running_loss += loss_sum.item() / target_length.item()\n","\n","    # 평균 loss를 계산하여 반환합니다.\n","    return valid_running_loss / len(valid_loader)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ra8NgFTOCy-p"},"source":["## 데이터 불러오기\n","\n","데이터를 불러올 때 우리는 먼저 언어별 사전을 정의하고 그 사전에 맞춰 데이터를 불러옵니다.\n","\n","- 문제 2. `valid_loader`를 불러오세요. 힌트) `train_loader`을 참고하세요."]},{"cell_type":"code","metadata":{"id":"9Jy8JWuPCyvC"},"source":["on_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# 사전 내 특수 단어의 index를 각각 미리 정의합니다.\n","SOS_token = 0\n","EOS_token = 1\n","UNK_token = 2\n","\n","# 데이터의 파일 정보\n","train_file_path = \"./train_en_ko.csv\"\n","valid_file_path = \"./valid_en_ko.csv\"\n","\n","max_len = 10\n","hidden_size = 256\n","\n","# 언어 별 사전 생성\n","eng_dic, kor_dic = make_dic(dataset_path=train_file_path)\n","\n","#  train, validation 데이터 csv 파일을 읽어옵니다.\n","train_loader = make_data_loader(train_file_path, eng_dic, kor_dic, max_len, 1)\n","\n","# <ToDo>: valid_dataset을 불러오세요.\n","# valid_loader = None\n","valid_loader = make_data_loader(valid_file_path, eng_dic, kor_dic, max_len, 1)\n","\n","# 영어와 한국어 사전의 크기(단어 개수)를 가져옵니다.\n","eng_dic_size = eng_dic.n_words\n","kor_dic_size = kor_dic.n_words"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RNlcKZKoDDEq"},"source":["## 모델 학습\n","\n","- 문제 3. `GRUMT` 인스턴스를 만드세요.\n","- 문제 4. `train` 함수를 이용하여 train data를 통해 모델 학습을 진행하세요."]},{"cell_type":"code","metadata":{"id":"BpjT1fyTDF15","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651111800145,"user_tz":-540,"elapsed":1992976,"user":{"displayName":"빅데이터학과/한대건","userId":"11786315672058136517"}},"outputId":"aa6fd956-1cc1-41ae-f1d9-49c9ee9469ac"},"source":["# <ToDo>: GRUMT 클래스의 인스턴스를 만드세요. 인스턴스 생성 시 필요한 parameter를 전달해주세요.\n","# model = GRUMT(None).to(on_device) # 'input_size', 'hidden_size', 'output_size', 'max_length', and 'device'\n","model = GRUMT(input_size=eng_dic_size, hidden_size=hidden_size, output_size=kor_dic_size, max_length=max_len,device=on_device).to(on_device)\n","\n","# Adam optimizier를 사용합니다.\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","# <ToDo>: 학습을 위해 train 함수의 적절한 parameter를 전달해주세요.\n","# train(None) # model, device, optimizer, train_loader, valid_loader, num_epochs\n","train(model,on_device, optimizer, train_loader, valid_loader, num_epochs=12)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1, Step 100, Train Loss: 4.1118, Valid Loss: 2.4209\n","Epoch 1, Step 200, Train Loss: 3.7155, Valid Loss: 3.1673\n","Epoch 1, Step 300, Train Loss: 3.8826, Valid Loss: 3.5898\n","Epoch 1, Step 400, Train Loss: 4.0701, Valid Loss: 3.7822\n","Epoch 1, Step 500, Train Loss: 4.0576, Valid Loss: 5.1401\n","Epoch 1, Step 600, Train Loss: 3.9726, Valid Loss: 3.3424\n","Epoch 1, Step 700, Train Loss: 4.0942, Valid Loss: 4.5867\n","Epoch 1, Step 800, Train Loss: 3.9842, Valid Loss: 3.5111\n","Epoch 1, Step 900, Train Loss: 4.1949, Valid Loss: 3.2352\n","Epoch 1, Step 1000, Train Loss: 3.5580, Valid Loss: 2.8275\n","Epoch 1, Step 1100, Train Loss: 3.7851, Valid Loss: 3.5148\n","Epoch 1, Step 1200, Train Loss: 4.3540, Valid Loss: 3.2491\n","Epoch 1, Step 1300, Train Loss: 3.6183, Valid Loss: 3.0503\n","Epoch 1, Step 1400, Train Loss: 3.7690, Valid Loss: 3.9024\n","Epoch 1, Step 1500, Train Loss: 4.2699, Valid Loss: 3.1968\n","Epoch 1, Step 1600, Train Loss: 3.8785, Valid Loss: 4.0699\n","Epoch 1, Step 1700, Train Loss: 4.4556, Valid Loss: 3.9169\n","Epoch 1, Step 1800, Train Loss: 4.1717, Valid Loss: 3.5760\n","Epoch 1, Step 1900, Train Loss: 3.8474, Valid Loss: 3.2623\n","Epoch 1, Step 2000, Train Loss: 4.2295, Valid Loss: 3.5532\n","Epoch 2, Step 2100, Train Loss: 3.5814, Valid Loss: 3.8372\n","Epoch 2, Step 2200, Train Loss: 3.7716, Valid Loss: 3.8656\n","Epoch 2, Step 2300, Train Loss: 3.3569, Valid Loss: 3.7077\n","Epoch 2, Step 2400, Train Loss: 3.3083, Valid Loss: 3.4013\n","Epoch 2, Step 2500, Train Loss: 3.5538, Valid Loss: 6.1016\n","Epoch 2, Step 2600, Train Loss: 3.4131, Valid Loss: 3.8786\n","Epoch 2, Step 2700, Train Loss: 3.0730, Valid Loss: 4.5338\n","Epoch 2, Step 2800, Train Loss: 3.3334, Valid Loss: 4.0627\n","Epoch 2, Step 2900, Train Loss: 3.5535, Valid Loss: 3.7216\n","Epoch 2, Step 3000, Train Loss: 3.5161, Valid Loss: 3.4136\n","Epoch 2, Step 3100, Train Loss: 3.4879, Valid Loss: 3.4263\n","Epoch 2, Step 3200, Train Loss: 3.4431, Valid Loss: 3.6951\n","Epoch 2, Step 3300, Train Loss: 3.4938, Valid Loss: 3.1246\n","Epoch 2, Step 3400, Train Loss: 3.2323, Valid Loss: 4.4658\n","Epoch 2, Step 3500, Train Loss: 3.4092, Valid Loss: 3.5171\n","Epoch 2, Step 3600, Train Loss: 3.3113, Valid Loss: 3.8980\n","Epoch 2, Step 3700, Train Loss: 3.5549, Valid Loss: 4.0538\n","Epoch 2, Step 3800, Train Loss: 3.6967, Valid Loss: 4.1579\n","Epoch 2, Step 3900, Train Loss: 3.5366, Valid Loss: 3.8998\n","Epoch 2, Step 4000, Train Loss: 3.2694, Valid Loss: 4.1910\n","Epoch 3, Step 4100, Train Loss: 3.1534, Valid Loss: 3.6464\n","Epoch 3, Step 4200, Train Loss: 3.3647, Valid Loss: 4.1514\n","Epoch 3, Step 4300, Train Loss: 2.9885, Valid Loss: 3.4749\n","Epoch 3, Step 4400, Train Loss: 2.8906, Valid Loss: 4.2330\n","Epoch 3, Step 4500, Train Loss: 2.8027, Valid Loss: 5.2413\n","Epoch 3, Step 4600, Train Loss: 3.2459, Valid Loss: 4.1882\n","Epoch 3, Step 4700, Train Loss: 2.4769, Valid Loss: 5.0951\n","Epoch 3, Step 4800, Train Loss: 2.9999, Valid Loss: 4.1724\n","Epoch 3, Step 4900, Train Loss: 2.8223, Valid Loss: 4.0048\n","Epoch 3, Step 5000, Train Loss: 2.5966, Valid Loss: 3.8758\n","Epoch 3, Step 5100, Train Loss: 2.6679, Valid Loss: 3.9711\n","Epoch 3, Step 5200, Train Loss: 2.7202, Valid Loss: 3.9840\n","Epoch 3, Step 5300, Train Loss: 2.5305, Valid Loss: 3.5130\n","Epoch 3, Step 5400, Train Loss: 2.6254, Valid Loss: 4.4634\n","Epoch 3, Step 5500, Train Loss: 2.7293, Valid Loss: 3.9211\n","Epoch 3, Step 5600, Train Loss: 2.7589, Valid Loss: 4.4854\n","Epoch 3, Step 5700, Train Loss: 3.0127, Valid Loss: 4.4948\n","Epoch 3, Step 5800, Train Loss: 2.7112, Valid Loss: 4.2515\n","Epoch 3, Step 5900, Train Loss: 3.0811, Valid Loss: 4.4473\n","Epoch 3, Step 6000, Train Loss: 2.7258, Valid Loss: 4.7033\n","Epoch 4, Step 6100, Train Loss: 2.5067, Valid Loss: 4.2609\n","Epoch 4, Step 6200, Train Loss: 2.5783, Valid Loss: 5.0044\n","Epoch 4, Step 6300, Train Loss: 2.3948, Valid Loss: 4.1892\n","Epoch 4, Step 6400, Train Loss: 2.4191, Valid Loss: 4.7671\n","Epoch 4, Step 6500, Train Loss: 2.2656, Valid Loss: 4.8520\n","Epoch 4, Step 6600, Train Loss: 2.4781, Valid Loss: 5.0015\n","Epoch 4, Step 6700, Train Loss: 2.1605, Valid Loss: 5.0830\n","Epoch 4, Step 6800, Train Loss: 2.5491, Valid Loss: 4.4430\n","Epoch 4, Step 6900, Train Loss: 2.3762, Valid Loss: 4.1770\n","Epoch 4, Step 7000, Train Loss: 2.1531, Valid Loss: 3.9997\n","Epoch 4, Step 7100, Train Loss: 1.9550, Valid Loss: 4.2988\n","Epoch 4, Step 7200, Train Loss: 2.3575, Valid Loss: 4.2374\n","Epoch 4, Step 7300, Train Loss: 1.7943, Valid Loss: 4.0327\n","Epoch 4, Step 7400, Train Loss: 1.9715, Valid Loss: 5.0988\n","Epoch 4, Step 7500, Train Loss: 1.9956, Valid Loss: 4.4367\n","Epoch 4, Step 7600, Train Loss: 2.2523, Valid Loss: 4.5065\n","Epoch 4, Step 7700, Train Loss: 2.4473, Valid Loss: 4.9104\n","Epoch 4, Step 7800, Train Loss: 2.3455, Valid Loss: 4.4470\n","Epoch 4, Step 7900, Train Loss: 2.3870, Valid Loss: 4.4473\n","Epoch 4, Step 8000, Train Loss: 2.0848, Valid Loss: 4.5021\n","Epoch 5, Step 8100, Train Loss: 1.8522, Valid Loss: 4.2380\n","Epoch 5, Step 8200, Train Loss: 2.0495, Valid Loss: 4.6449\n","Epoch 5, Step 8300, Train Loss: 1.9995, Valid Loss: 4.5593\n","Epoch 5, Step 8400, Train Loss: 2.0308, Valid Loss: 4.8349\n","Epoch 5, Step 8500, Train Loss: 1.9105, Valid Loss: 5.1252\n","Epoch 5, Step 8600, Train Loss: 2.1003, Valid Loss: 4.7109\n","Epoch 5, Step 8700, Train Loss: 1.6282, Valid Loss: 5.0778\n","Epoch 5, Step 8800, Train Loss: 2.0989, Valid Loss: 4.5622\n","Epoch 5, Step 8900, Train Loss: 1.8407, Valid Loss: 4.3125\n","Epoch 5, Step 9000, Train Loss: 1.5811, Valid Loss: 4.1961\n","Epoch 5, Step 9100, Train Loss: 1.4995, Valid Loss: 4.4417\n","Epoch 5, Step 9200, Train Loss: 1.6746, Valid Loss: 4.5566\n","Epoch 5, Step 9300, Train Loss: 1.3214, Valid Loss: 4.1969\n","Epoch 5, Step 9400, Train Loss: 1.4641, Valid Loss: 4.5194\n","Epoch 5, Step 9500, Train Loss: 1.5010, Valid Loss: 4.7048\n","Epoch 5, Step 9600, Train Loss: 1.7455, Valid Loss: 5.1587\n","Epoch 5, Step 9700, Train Loss: 1.9732, Valid Loss: 5.1563\n","Epoch 5, Step 9800, Train Loss: 1.9646, Valid Loss: 4.7823\n","Epoch 5, Step 9900, Train Loss: 1.8947, Valid Loss: 4.7975\n","Epoch 5, Step 10000, Train Loss: 1.9696, Valid Loss: 4.9425\n","Epoch 6, Step 10100, Train Loss: 1.5460, Valid Loss: 4.6571\n","Epoch 6, Step 10200, Train Loss: 1.8594, Valid Loss: 4.9515\n","Epoch 6, Step 10300, Train Loss: 1.7555, Valid Loss: 5.1253\n","Epoch 6, Step 10400, Train Loss: 1.7064, Valid Loss: 5.0464\n","Epoch 6, Step 10500, Train Loss: 1.6113, Valid Loss: 5.0118\n","Epoch 6, Step 10600, Train Loss: 1.8494, Valid Loss: 4.9863\n","Epoch 6, Step 10700, Train Loss: 1.5954, Valid Loss: 5.2375\n","Epoch 6, Step 10800, Train Loss: 1.8461, Valid Loss: 5.2445\n","Epoch 6, Step 10900, Train Loss: 1.6625, Valid Loss: 4.7756\n","Epoch 6, Step 11000, Train Loss: 1.4321, Valid Loss: 4.7202\n","Epoch 6, Step 11100, Train Loss: 1.3654, Valid Loss: 4.7675\n","Epoch 6, Step 11200, Train Loss: 1.5575, Valid Loss: 5.0927\n","Epoch 6, Step 11300, Train Loss: 1.1603, Valid Loss: 4.6941\n","Epoch 6, Step 11400, Train Loss: 1.4634, Valid Loss: 5.2343\n","Epoch 6, Step 11500, Train Loss: 1.3998, Valid Loss: 5.0017\n","Epoch 6, Step 11600, Train Loss: 1.4126, Valid Loss: 4.9912\n","Epoch 6, Step 11700, Train Loss: 1.7978, Valid Loss: 5.1727\n","Epoch 6, Step 11800, Train Loss: 1.5967, Valid Loss: 5.2042\n","Epoch 6, Step 11900, Train Loss: 1.6415, Valid Loss: 5.1665\n","Epoch 6, Step 12000, Train Loss: 1.6175, Valid Loss: 5.2755\n","Epoch 7, Step 12100, Train Loss: 1.4201, Valid Loss: 5.2473\n","Epoch 7, Step 12200, Train Loss: 1.5858, Valid Loss: 5.3685\n","Epoch 7, Step 12300, Train Loss: 1.6194, Valid Loss: 5.4799\n","Epoch 7, Step 12400, Train Loss: 1.5940, Valid Loss: 5.3660\n","Epoch 7, Step 12500, Train Loss: 1.5262, Valid Loss: 5.3261\n","Epoch 7, Step 12600, Train Loss: 1.6265, Valid Loss: 5.6167\n","Epoch 7, Step 12700, Train Loss: 1.2944, Valid Loss: 5.5821\n","Epoch 7, Step 12800, Train Loss: 1.5290, Valid Loss: 5.5234\n","Epoch 7, Step 12900, Train Loss: 1.6236, Valid Loss: 5.1447\n","Epoch 7, Step 13000, Train Loss: 1.3404, Valid Loss: 4.9499\n","Epoch 7, Step 13100, Train Loss: 1.2522, Valid Loss: 5.0280\n","Epoch 7, Step 13200, Train Loss: 1.3491, Valid Loss: 5.2812\n","Epoch 7, Step 13300, Train Loss: 1.1470, Valid Loss: 4.9093\n","Epoch 7, Step 13400, Train Loss: 1.2603, Valid Loss: 5.5462\n","Epoch 7, Step 13500, Train Loss: 1.4258, Valid Loss: 5.2761\n","Epoch 7, Step 13600, Train Loss: 1.3649, Valid Loss: 5.2395\n","Epoch 7, Step 13700, Train Loss: 1.7298, Valid Loss: 5.5124\n","Epoch 7, Step 13800, Train Loss: 1.5641, Valid Loss: 5.6171\n","Epoch 7, Step 13900, Train Loss: 1.4368, Valid Loss: 5.4729\n","Epoch 7, Step 14000, Train Loss: 1.4254, Valid Loss: 5.5501\n","Epoch 8, Step 14100, Train Loss: 1.1976, Valid Loss: 5.3118\n","Epoch 8, Step 14200, Train Loss: 1.5209, Valid Loss: 5.4760\n","Epoch 8, Step 14300, Train Loss: 1.4801, Valid Loss: 5.7019\n","Epoch 8, Step 14400, Train Loss: 1.4966, Valid Loss: 5.6039\n","Epoch 8, Step 14500, Train Loss: 1.2607, Valid Loss: 5.5751\n","Epoch 8, Step 14600, Train Loss: 1.4272, Valid Loss: 5.8191\n","Epoch 8, Step 14700, Train Loss: 1.3587, Valid Loss: 5.5540\n","Epoch 8, Step 14800, Train Loss: 1.3793, Valid Loss: 5.6832\n","Epoch 8, Step 14900, Train Loss: 1.3240, Valid Loss: 5.5980\n","Epoch 8, Step 15000, Train Loss: 1.2619, Valid Loss: 5.1559\n","Epoch 8, Step 15100, Train Loss: 1.0138, Valid Loss: 5.2431\n","Epoch 8, Step 15200, Train Loss: 1.2047, Valid Loss: 5.4625\n","Epoch 8, Step 15300, Train Loss: 0.9512, Valid Loss: 5.2024\n","Epoch 8, Step 15400, Train Loss: 1.1031, Valid Loss: 5.6183\n","Epoch 8, Step 15500, Train Loss: 1.2147, Valid Loss: 5.5747\n","Epoch 8, Step 15600, Train Loss: 1.1894, Valid Loss: 5.3579\n","Epoch 8, Step 15700, Train Loss: 1.5285, Valid Loss: 5.6966\n","Epoch 8, Step 15800, Train Loss: 1.3281, Valid Loss: 6.0754\n","Epoch 8, Step 15900, Train Loss: 1.3709, Valid Loss: 5.5984\n","Epoch 8, Step 16000, Train Loss: 1.2933, Valid Loss: 5.5548\n","Epoch 9, Step 16100, Train Loss: 1.1415, Valid Loss: 5.2735\n","Epoch 9, Step 16200, Train Loss: 1.4789, Valid Loss: 5.6972\n","Epoch 9, Step 16300, Train Loss: 1.4547, Valid Loss: 5.9469\n","Epoch 9, Step 16400, Train Loss: 1.2035, Valid Loss: 5.7379\n","Epoch 9, Step 16500, Train Loss: 1.2347, Valid Loss: 5.8096\n","Epoch 9, Step 16600, Train Loss: 1.4481, Valid Loss: 5.6805\n","Epoch 9, Step 16700, Train Loss: 1.1780, Valid Loss: 5.5742\n","Epoch 9, Step 16800, Train Loss: 1.2798, Valid Loss: 5.6625\n","Epoch 9, Step 16900, Train Loss: 1.2556, Valid Loss: 5.4271\n","Epoch 9, Step 17000, Train Loss: 1.0063, Valid Loss: 5.3689\n","Epoch 9, Step 17100, Train Loss: 0.9551, Valid Loss: 5.2027\n","Epoch 9, Step 17200, Train Loss: 0.9819, Valid Loss: 5.2822\n","Epoch 9, Step 17300, Train Loss: 0.8214, Valid Loss: 5.4311\n","Epoch 9, Step 17400, Train Loss: 0.9536, Valid Loss: 5.6650\n","Epoch 9, Step 17500, Train Loss: 0.9928, Valid Loss: 5.5323\n","Epoch 9, Step 17600, Train Loss: 1.1149, Valid Loss: 5.6421\n","Epoch 9, Step 17700, Train Loss: 1.3098, Valid Loss: 6.0385\n","Epoch 9, Step 17800, Train Loss: 1.1830, Valid Loss: 6.0583\n","Epoch 9, Step 17900, Train Loss: 1.1478, Valid Loss: 5.8623\n","Epoch 9, Step 18000, Train Loss: 1.2587, Valid Loss: 5.7272\n","Epoch 10, Step 18100, Train Loss: 0.9656, Valid Loss: 5.7411\n","Epoch 10, Step 18200, Train Loss: 1.3035, Valid Loss: 5.7935\n","Epoch 10, Step 18300, Train Loss: 1.1933, Valid Loss: 5.9782\n","Epoch 10, Step 18400, Train Loss: 1.1309, Valid Loss: 6.0788\n","Epoch 10, Step 18500, Train Loss: 1.1443, Valid Loss: 6.0833\n","Epoch 10, Step 18600, Train Loss: 1.4004, Valid Loss: 6.3901\n","Epoch 10, Step 18700, Train Loss: 1.1333, Valid Loss: 6.1312\n","Epoch 10, Step 18800, Train Loss: 1.1345, Valid Loss: 6.1437\n","Epoch 10, Step 18900, Train Loss: 1.0823, Valid Loss: 5.9254\n","Epoch 10, Step 19000, Train Loss: 1.0842, Valid Loss: 5.7337\n","Epoch 10, Step 19100, Train Loss: 0.8268, Valid Loss: 5.5729\n","Epoch 10, Step 19200, Train Loss: 0.9473, Valid Loss: 5.6859\n","Epoch 10, Step 19300, Train Loss: 0.6678, Valid Loss: 5.8651\n","Epoch 10, Step 19400, Train Loss: 0.8818, Valid Loss: 5.7888\n","Epoch 10, Step 19500, Train Loss: 0.9251, Valid Loss: 5.7167\n","Epoch 10, Step 19600, Train Loss: 0.9528, Valid Loss: 5.7421\n","Epoch 10, Step 19700, Train Loss: 1.2129, Valid Loss: 6.2941\n","Epoch 10, Step 19800, Train Loss: 1.0283, Valid Loss: 6.4207\n","Epoch 10, Step 19900, Train Loss: 1.1685, Valid Loss: 5.9023\n","Epoch 10, Step 20000, Train Loss: 1.1183, Valid Loss: 6.0512\n","Epoch 11, Step 20100, Train Loss: 0.9442, Valid Loss: 5.8494\n","Epoch 11, Step 20200, Train Loss: 1.1494, Valid Loss: 5.8978\n","Epoch 11, Step 20300, Train Loss: 1.1755, Valid Loss: 5.8945\n","Epoch 11, Step 20400, Train Loss: 1.0718, Valid Loss: 6.1372\n","Epoch 11, Step 20500, Train Loss: 1.0794, Valid Loss: 5.9558\n","Epoch 11, Step 20600, Train Loss: 1.2241, Valid Loss: 6.3460\n","Epoch 11, Step 20700, Train Loss: 1.0025, Valid Loss: 6.1787\n","Epoch 11, Step 20800, Train Loss: 1.0235, Valid Loss: 6.1761\n","Epoch 11, Step 20900, Train Loss: 1.0422, Valid Loss: 6.0269\n","Epoch 11, Step 21000, Train Loss: 0.9184, Valid Loss: 5.8374\n","Epoch 11, Step 21100, Train Loss: 0.7332, Valid Loss: 5.6382\n","Epoch 11, Step 21200, Train Loss: 0.8855, Valid Loss: 5.7582\n","Epoch 11, Step 21300, Train Loss: 0.7053, Valid Loss: 5.7816\n","Epoch 11, Step 21400, Train Loss: 0.7776, Valid Loss: 6.0064\n","Epoch 11, Step 21500, Train Loss: 0.8593, Valid Loss: 6.0694\n","Epoch 11, Step 21600, Train Loss: 0.9276, Valid Loss: 5.9946\n","Epoch 11, Step 21700, Train Loss: 1.0809, Valid Loss: 6.2937\n","Epoch 11, Step 21800, Train Loss: 0.9215, Valid Loss: 6.3622\n","Epoch 11, Step 21900, Train Loss: 0.9527, Valid Loss: 6.2066\n","Epoch 11, Step 22000, Train Loss: 1.0161, Valid Loss: 6.0896\n","Epoch 12, Step 22100, Train Loss: 0.7440, Valid Loss: 6.1447\n","Epoch 12, Step 22200, Train Loss: 1.1067, Valid Loss: 6.0911\n","Epoch 12, Step 22300, Train Loss: 1.0468, Valid Loss: 6.3380\n","Epoch 12, Step 22400, Train Loss: 0.9628, Valid Loss: 6.3188\n","Epoch 12, Step 22500, Train Loss: 0.9800, Valid Loss: 6.2780\n","Epoch 12, Step 22600, Train Loss: 1.1405, Valid Loss: 6.5255\n","Epoch 12, Step 22700, Train Loss: 0.9671, Valid Loss: 6.4795\n","Epoch 12, Step 22800, Train Loss: 1.0357, Valid Loss: 6.4370\n","Epoch 12, Step 22900, Train Loss: 0.9146, Valid Loss: 6.1115\n","Epoch 12, Step 23000, Train Loss: 0.8612, Valid Loss: 6.1979\n","Epoch 12, Step 23100, Train Loss: 0.6485, Valid Loss: 5.7712\n","Epoch 12, Step 23200, Train Loss: 0.7946, Valid Loss: 6.0368\n","Epoch 12, Step 23300, Train Loss: 0.6337, Valid Loss: 6.1861\n","Epoch 12, Step 23400, Train Loss: 0.7831, Valid Loss: 6.2129\n","Epoch 12, Step 23500, Train Loss: 0.7573, Valid Loss: 5.9586\n","Epoch 12, Step 23600, Train Loss: 0.8762, Valid Loss: 6.0838\n","Epoch 12, Step 23700, Train Loss: 1.0374, Valid Loss: 6.5406\n","Epoch 12, Step 23800, Train Loss: 0.9883, Valid Loss: 6.5021\n","Epoch 12, Step 23900, Train Loss: 0.8973, Valid Loss: 6.2281\n","Epoch 12, Step 24000, Train Loss: 0.9756, Valid Loss: 6.1757\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"kOamjj9A0y__"},"execution_count":null,"outputs":[]}]}